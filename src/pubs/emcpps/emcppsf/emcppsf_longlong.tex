\newpage
\section[{\tt long}~{\tt long}]{{\SecCode long}~{\SecCode long}}\label{long-long}


\texttt{long long} is a \textbf{fundamental integral type} guaranteed to have (at least) 64 bits on all
platforms.

\subsection[Description]{Description}\label{description}

The \textbf{integral type} \texttt{long}~\texttt{long} and its companion
type \texttt{unsigned}~\texttt{long}~\texttt{long} are the only two
\textbf{fundamental integral types} in C++ that are guaranteed to have
at least 64~bits on all conforming platforms{\cprotect\footnote{\texttt{long}~\texttt{long}
has been available in C since the C99 standard, and many C++ compilers
supported it as an extension prior to C++11.}}:

\begin{lstlisting}[language=C++]
#include <climits>  // (ù{\codeincomments{CHAR\_BIT}}ù) (a.k.a.~8, see below)

long long          a;  // (ù{\codeincomments{sizeof(a) * CHAR\_BIT >= 64}}ù)
unsigned long long b;  // (ù{\codeincomments{sizeof(b) * CHAR\_BIT >= 64}}ù)

static_assert(sizeof(a) == sizeof(b), "");
// I.e., (ù{\codeincomments{a}}ù) and (ù{\codeincomments{b}}ù) necessarily have the same size in every program.
\end{lstlisting}

\noindent On all conforming platforms, \texttt{CHAR\_BIT} --- the number of bits
in a byte --- is at least~8 and, on virtually all commonly available
commercial platforms today, is exactly~8, as is
\texttt{sizeof(long}~\texttt{long)}.

The corresponding integer-literal suffixes indicating type
\texttt{long}~\texttt{long} are \texttt{ll} and \texttt{LL}; for
\texttt{unsigned}~\texttt{long}~\texttt{long}, any of eight alternatives
are accepted: \texttt{ull}, \texttt{ULL}, \texttt{uLL}, \texttt{Ull},
\texttt{llu}, \texttt{LLU}, \texttt{LLu},
\texttt{llU}{\cprotect\footnote{Note that \texttt{long}~\texttt{long}
and \texttt{unsigned}~\texttt{long}~\texttt{long} are also candidates
for the type of an integer literal having a large enough value. As an
example, the type of the literal \texttt{2147483648} (one more than
the upper bound of a 32-bit integer) is likely to be
\texttt{long}~\texttt{long} on a 32-bit platform.}}:

\begin{lstlisting}[language=C++]
auto i = 0LL;  // (ù{\codeincomments{long long}}ù), (ù{\codeincomments{sizeof(i) * CHAR\_BIT >= 64}}ù)
auto u = 0uLL  // (ù{\codeincomments{unsigned long long}}ù), (ù{\codeincomments{sizeof(u) * CHAR\_BIT >= 64}}ù)
\end{lstlisting}

\noindent For a historical perspective on how integral types have evolved (and
continue to evolve) over time, see {\it\titleref{historical-perspective-on-the-evolution-of-use-of-fundamental-integral-types}} on page~\pageref{historical-perspective-on-the-evolution-of-use-of-fundamental-integral-types}.

\subsection[Use Cases]{Use Cases}\label{use-cases}

\subsubsection[When your pedestrian four-byte {\tt int} might not cut it]{When your pedestrian four-byte {\SubsubsecCode int} might not cut it}\label{when-your-pedestrian-four-byte-int-might-not-cut-it}

Deciding when an \texttt{int} (i.e., exactly 32~bits) is big enough is
often a nonissue. For most common things we deal with day to day ---
miles on our car, years of age, bottles of wine --- having more than about a
billion of them just isn't worth thinking about, at least not in the
interface.{\cprotect\footnote{For efficient storage in a \texttt{class}
or \texttt{struct}, however, we may well decide to represent such
quantities more compactly using a \texttt{short} or \texttt{char}; see
also the aliases found in C++11's \texttt{<cstdint>}.}} Sometimes the size of the virtual address space for the
underlying architecture itself dictates how large an integer you will
need. For example, specifying the \emph{distance} between two pointers
into a contiguous array or the size of the array itself could, on a
64-bit platform, well exceed the size of an \texttt{int} or
\texttt{unsigned}~\texttt{int}, respectively. Using either
\texttt{long}~\texttt{long} or
\texttt{unsigned}~\texttt{long}~\texttt{long} here would, however, not
be indicated as the respective platform-dependent integer types
(\texttt{typedef}s) \texttt{std::ptrdiff\_t} and \texttt{std::size\_t}
are provided expressly for such use (and avoid wasting space where it
cannot be used by the underlying hardware).

Occasionally, however, the decision of whether to use an \texttt{int} is
neither platform dependent nor clear cut, in which case using an
\texttt{int} is almost certainly a bad idea. As part of a financial
library, suppose we were asked to provide a function that, given a date,
returns the number of shares of some particular stock, identified by its
security id (\texttt{SecId}) traded on the New York Stock Exchange
(NYSE).\footnote{The NYSE consists of 2400 different (Equity) securities. The average daily trading volume (the number of shares traded on a given day) on the NYSE is typically  between 2? and 6? billion shares per day, with the maximum volume reaching ??? on ???. (TODO, TBD -- will fill this in much later. NOTE: from Harry: ``it looks like the numbers you are referencing are for the NYSE Composite -- i.e., the volume of all shares traded in companies listed at the NYSE. That number is significantly higher than the number of shares traded on the NYSE exchange. That is because NYSE shares can trade on other exchanges. With that proviso, over the past 5 years, NYSE Composite trading volume has averaged 3.8 billion shares per day. The highest volume day was March 20, 2020, when just over 9 billion shares were traded.")} Since the average daily rate
for even the most heavily traded stocks (roughly 70 million) appears to
be well under the maximum value a signed \texttt{int} supports (more
than 2 billion), we might at first think to write the function
returning an \texttt{int}:

\begin{lstlisting}[language=C++]
int volYMD(SecId equity, int year, int month, int day);  // (1) BAD IDEA
\end{lstlisting}

\noindent One obvious problem with this interface is that the daily fluctuations
in turbulent times might exceed the maximum value representable by a
32-bit \texttt{int}, which, unless detected internally, would result in
\textbf{signed integer overflow}, which is both \textbf{undefined
behavior} and a potential security hole.{\cprotect\footnote{\textbf{Signed
integer overflow} is among the most pervasive kinds of defects
enabling avenues of deliberate attack from outside sources. For an overview of integer overflow in C++, see \textbf{ballman}. For a more focused discussion of secure coding in CPP using CERT standards, see \textbf{seacord13}, section x, pp. yy-zz.}} What's more, the growth rate of some companies,
especially technology companies, such as AAPL, GOOG, FB, AMZN, and MSFT, has
been at times seemingly exponential. To gain an extra
insurance factor of two, we might opt to replace the return type
\texttt{int} with an \texttt{unsigned}~\texttt{int}:

\begin{lstlisting}[language=C++]
unsigned volYND(SecId stock, int year, int month, int day);  // (2) BAD IDEA!
\end{lstlisting}

\noindent Use of an \texttt{unsigned}~\texttt{int}, however, simply delays the
inevitable as the number of shares being traded is almost certainly
going to grow over time.

Furthermore, the algebra for unsigned quantities is entirely different
from what one would normally expect from an \texttt{int}. For example, if we
were to try to express the day-over-day increase in volume by
subtracting two calls to this function and if the number of shares
traded were to have decreased, then the \texttt{unsigned}~\texttt{int}
difference would wrap and the result would be a (typically) large
unsigned \textbf{garbage value}.{\cprotect\footnote{Because integer
literals are themselves of type \texttt{int} and not \texttt{unsigned},
comparing an unsigned value with a negative signed one does not
typically go well; hence, many compilers will warn when the two types
are mixed, which itself is problematic.}}

If we happen to be on a 64-bit platform, we might choose to return a
\texttt{long}:

\begin{lstlisting}[language=C++]
long volYMD(SecId stock, int year, int month, int day);  // (3) NOT A GOOD IDEA
\end{lstlisting}

\noindent The problems using \texttt{long} as the return type are that it (1) is
not (yet) generally considered a \textbf{vocabulary type} (see
{\it\titleref{longlong-appendix}} on page~\pageref{longlong-appendix}), and (2) would reduce portability (see
{\it\titleref{longlong-potential-pitfalls}: \titleref{relying-on-the-relative-sizes-of-int,-long,-and-long-long}} on page~\pageref{relying-on-the-relative-sizes-of-int,-long,-and-long-long}).

Prior to C++11, we might have considered returning a \texttt{double}:

\begin{lstlisting}[language=C++]
double volYMD(SecId stock, int year, int month, int day);  // (4) OK
\end{lstlisting}

\noindent At least with \texttt{double} we know that we will have (at no
additional size) sufficient precision (53 bits) to express
integers accurately into the quadrillions, which will certainly cover us
for any foreseeable future. The main drawback is that \texttt{double}
doesn't properly describe the nature of the type that we are returning
--- i.e., a whole integer number of shares --- and so its algebra,
although not as dubious as \texttt{unsigned}~\texttt{int}, isn't ideal
either.

With the advent of C++11, we might consider using one of the type aliases
in \texttt{<cstdint>}:

\begin{lstlisting}[language=C++]
std::int64_t volYMD(SecId stock, int year, int month, int day);  // (4) OK
\end{lstlisting}

\noindent This choice addresses most of the issues discussed above except that,
instead of being a specific C++ type, it is a platform-dependent alias
that is likely to be a \texttt{long} on a 64-bit platform and almost
certainly a \texttt{long}~\texttt{long} on a 32-bit one. Such exact size
requirements are often necessary for packing data in structures and
arrays but are not as useful when reasoning about them in the
interfaces of functions where having a common set of fundamental
\textbf{vocabulary types} becomes much more important (e.g., for
interoperability).

All of this leads us to our final alternative, \texttt{long}~\texttt{long}:

\begin{lstlisting}[language=C++]
long long volYMD(SecId stock, int year, int month, int day);  // (5) GOOD IDEA
\end{lstlisting}

\noindent In addition to being a signed fundamental integral type of sufficient
capacity on all platforms, \texttt{long}~\texttt{long} is the same C++
type \emph{relative} to other C++ types on all platforms.

\subsection[Potential Pitfalls]{Potential Pitfalls}\label{longlong-potential-pitfalls}

\subsubsection[Relying on the relative sizes of {\tt int}, {\tt long}, and {\tt long}~{\tt long}]{Relying on the relative sizes of {\SubsubsecCode int}, {\SubsubsecCode long}, and {\SubsubsecCode long}~{\SubsubsecCode long}}\label{relying-on-the-relative-sizes-of-int,-long,-and-long-long}

As discussed at some length in {\it\titleref{longlong-appendix}} on page~\pageref{longlong-appendix}, the
fundamental integral types have historically been a moving target. On
older, 32-bit platforms, a \texttt{long} was often 32~bits and, prior to
C++11, a (nonstandard) \texttt{long}~\texttt{long} (or its
platform-dependent equivalent) was needed to ensure that 64~bits were
available. When the correctness of code depends on either
\texttt{sizeof(int)}~\texttt{<}~\texttt{sizeof(long)} or
\texttt{sizeof(long)}~\texttt{<}~\texttt{sizeof(long}~\texttt{long)},
portability is needlessly restricted. Relying instead on only the
guaranteed{\cprotect\footnote{Due to the unfathomable amount of software
that would stop working if an \texttt{int} were ever anything but
exactly \emph{four} bytes, we --- along with the late Richard Stevens
of Unix fame (see \textbf{{stevens93}}, section~2.5.1., pp.~31--32, specifically row~6, column~4, Figure~2.2, p.~32) --- are prepared
to \emph{guarantee} that it will never become as large as a
\texttt{long}~\texttt{long} for any general-purpose computer.}}
property that
\texttt{sizeof(int)} \texttt{<} \texttt{sizeof(long} \texttt{long)}
avoids such portability issues since the relative sizes of the
(fundamental) \texttt{long} and \texttt{long}~\texttt{long} integral
types continue to evolve.

When precise control of size \emph{in the implementation} (as opposed to
in the interface) matters, consider using one of the standard signed
(\texttt{int*n*\_t}) or unsigned (\texttt{uint*n*\_t}) integer aliases
(\texttt{typedef}s) provided (since C++11) in \texttt{<cstdint>} and
summarized here in Table~\ref{longlong-table1}.\newpage

\begin{table}[h!]
\begin{center}
\begin{threeparttable}
\caption{Useful {\ttfamily\bfseries typedef}s found in {\ttfamily\bfseries <cstdint>} (since
C++11)}\label{longlong-table1} \vspace{1.5ex}
{\small \begin{tabular}{c|c|c}\thickhline
\rowcolor[gray]{.9} {\sffamily\bfseries Exact Size} & {\sffamily\bfseries Fastest (signed) integral type } & {\sffamily\bfseries Smallest (signed) integer type }\\
\rowcolor[gray]{.9}  & {\sffamily\bfseries having at least N bits} & {\sffamily\bfseries  having at least N bits}\\\hline
{\tt int8\_t} & {\tt int\_fast8\_t} & {\tt int\_least8\_t}\\ \hline
{\tt int16\_t}\tnote{a} & {\tt int\_fast16\_t} & {\tt int\_least16\_t}\\ \hline
{\tt int32\_t} & {\tt int\_fast32\_t} & {\tt int\_least32\_t}\\ \hline
{\tt int64\_t} & {\tt int\_fast64\_t} & {\tt int\_least64\_t}\\ \thickhline
\end{tabular} }
\begin{tablenotes}{\footnotesize
\item[a]{optional\\
Note: Also see {\tt intmax\_t}, the maximum width integer type, which might be none of the above.}
}
\end{tablenotes}
\end{threeparttable}
\end{center}
\end{table}

\subsection[See Also]{See Also}\label{see-also}

\begin{itemize}
\item{Section~\ref{digit-separators}, ``\titleref{digit-separators}" — Safe C++11 feature that can help with visually separating digits of large \texttt{long}~\texttt{long} literals}
\item{Section~\ref{binary-literals}, ``\titleref{binary-literals}" — Safe C++11 feature that allows programmers to specify binary constants directly in the source code; large binary values might only fit in a \texttt{long}~\texttt{long} or even \texttt{unsigned}~\texttt{long}~\texttt{long}}
\end{itemize}

\subsection[Further Reading]{Further Reading}\label{further-reading}

None so far

\subsection[Appendix: Historical Perspective on the Evolution of Use of Fundamental Integral Types]{Appendix: Historical Perspective on the Evolution of Use of Fundamental Integral Types}\label{longlong-appendix}
\label{historical-perspective-on-the-evolution-of-use-of-fundamental-integral-types}

The designers of C got it right back in 1972 when
they created a portable \texttt{int} type that could act as a bridge
from a single-word (16-bit) integer, \texttt{short}, to a double-word
(32-bit) integer, \texttt{long}. Just by using \texttt{int}, one would
get the optimal space versus speed trade-off as the 32-bit computer \emph{word}
was on its way to becoming the norm.{\cprotect\footnote{The Motorola
68000 series (c.~1979) was a hybrid \emph{CISC} architecture employing
a 32-bit instruction set with 32-bit registers and a 32-bit external
data bus; internally, however, it used only 16-bit ALUs and a 16-bit
data bus.}}

During the late 1980s and into the 1990s, the word size of the machine and
the size of an \texttt{int} were synonymous.{\cprotect\footnote{Some of
the earlier mainframe computers, such as IBM 701 (c.~1954), had a word
size of 36 characters (1) to allow accurate representation of a signed
10-digit decimal number or (2) to hold up to six 6-bit characters.
Smaller computers, such as Digital Equipment Corporation's
PDP-1/PDP-9/PDP-15 used 18-bit words (so a double word held 36-bits);
memory addressing, however, was limited to just 12--18 bits (i.e., a
maximum 4K--256K 18-bit words of \emph{DRAM}). With the standardization
of 7-bit ASCII (c.~1967), its adoption throughout the 1970s, and its most
recent update (c.~1986), the common typical notion of character size
moved from 6 to 7 bytes. Some early conforming implementations (of C)
would choose to set \texttt{CHAR\_BIT} to 9 to allow two characters
per half word. (On some early vector-processing computers,
\texttt{CHAR\_BIT} is 32, making every type, including a
\texttt{char}, at least a 32-bit quantity.) As double-precision
floating (and floating-point coprocessors) for type \texttt{double}
became typical for scientific calculations, machine architectures
naturally evolved from 9-, 18-, and 36-bit words to the familiar 8-,
16-, 32-, and now 64-bit addressable integer words we have today.
Apart from embedded systems and \emph{DSPs}, a \texttt{char} is now
almost universally considered to be exactly 8 bits. Instead of
scrupulously and actively using \texttt{CHAR\_BIT} for the number of bits
in a \texttt{char}, consider statically asserting it instead:

\begin{lstlisting}[language=C++, basicstyle={\ttfamily\footnotesize}]
static_assert(CHAR_BIT == 8, "A (ù{\codeincomments{char}}ù) is not 8-bits on this CrAzY platform!");
\end{lstlisting}\vspace*{-1ex}
}} As cost of \emph{main memory} was decreasing exponentially
throughout the final two decades of the 20th
century,{\cprotect\footnote{Moore's law (c.~1965) --- the observation
that the number of transistors in densely packed integrated circuits
(e.g., DRAM) grows exponentially over time, doubling every 1--2 years
or so --- held for nearly a half century, until finally saturating in
the 2010s.}} the need for a much larger \emph{virtual address space}
quickly followed. Intel began its work on 64-bit architectures in the
early 1990s and realized one a decade later. As we progressed into
the 2000s, the common notion of \emph{word size} --- i.e., the width (in
bits) of typical registers within the CPU itself --- began to shift from
``the size of an \texttt{int}'' to ``the size of a simple (nonmember)
pointer type,'' e.g., \texttt{8}~\texttt{*}~\texttt{sizeof(void*)}, on the host platform. By this time, 16-bit \texttt{int} types (like
16-bit architectures) were long gone, but \texttt{long} was still
expected to be 32 bits on a 32-bit platform.{\cprotect\footnote{Sadly,
\texttt{long} was often used (improperly) to hold an address; hence,
the size of {\tt long} is associated with a de facto need (due to immeasurable amounts of legacy code) to remain in lockstep with pointer size.}}

Something new was needed to mean at least 64-bits on all platforms.
Enter \texttt{long}~\texttt{long}. We have now come full circle. On
64-bit platforms, an \texttt{int} is still 4~bytes, but a \texttt{long}
is now --- for practical reasons --- typically 8~bytes unless requested
explicitly{\cprotect\footnote{On 64-bit systems, \texttt{sizeof(long)}
is typically 8~bytes. Compiling with the \texttt{-m32} flag on either
GCC or Clang emulates compiling on a 32-bit platform:
\texttt{sizeof(long)} is likely to be~4, while
\texttt{sizeof(long}~\texttt{long)} remains~8.}} to be otherwise. To ensure portability until 32-bit machines go the way of 16-bit ones,
we have \texttt{long}~\texttt{long} to (1) provide a common \emph{vocabulary type}, (2)
make our intent clear, and (3) avoid the portability issue for
at least the next decade or two; still, see {\it\titleref{longlong-potential-pitfalls}: \titleref{relying-on-the-relative-sizes-of-int,-long,-and-long-long}} on page~\pageref{relying-on-the-relative-sizes-of-int,-long,-and-long-long} for some alternative ideas.



