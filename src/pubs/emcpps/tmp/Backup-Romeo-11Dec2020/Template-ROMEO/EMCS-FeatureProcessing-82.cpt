Some of
the earlier mainframe computers, such as IBM 701 (c.~1954), had a word
size of 36 characters (1) to allow accurate representation of a signed
10-digit decimal number or (2) to hold up to six 6-bit characters.
Smaller computers, such as Digital Equipment Corporation's
PDP-1/PDP-9/PDP-15 used 18-bit words (so a double word held 36-bits);
memory addressing, however, was limited to just 12--18 bits (i.e., a
maximum 4K--256K 18-bit words of \emph{DRAM}). With the standardization
of 7-bit ASCII (c.~1967), its adoption throughout the 1970s, and its most
recent update (c.~1986), the common typical notion of character size
moved from 6 to 7 bytes. Some early conforming implementations (of C)
would choose to set \texttt{CHAR\_BIT} to 9 to allow two characters
per half word. (On some early vector-processing computers,
\texttt{CHAR\_BIT} is 32, making every type, including a
\texttt{char}, at least a 32-bit quantity.) As double-precision
floating (and floating-point coprocessors) for type \texttt{double}
became typical for scientific calculations, machine architectures
naturally evolved from 9-, 18-, and 36-bit words to the familiar 8-,
16-, 32-, and now 64-bit addressable integer words we have today.
Apart from embedded systems and \emph{DSPs}, a \texttt{char} is now
almost universally considered to be exactly 8 bits. Instead of
scrupulously and actively using \texttt{CHAR\_BIT} for the number of bits
in a \texttt{char}, consider statically asserting it instead:

\begin{lstlisting}[language=C++, basicstyle={\ttfamily\footnotesize}]
static_assert(CHAR_BIT == 8, "A (ù{\codeincomments{char}}ù) is not 8-bits on this CrAzY platform!");
\end{lstlisting}\vspace*{-1ex}
      ^^E^^L 
